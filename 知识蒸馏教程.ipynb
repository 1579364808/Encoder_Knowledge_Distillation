{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForSequenceClassification  # 同样需要导入这个\n",
    "from transformers import AutoTokenizer, DebertaV2Model, DebertaV2Config\n",
    "from transformers import TrainingArguments\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2Encoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer"
   ],
   "id": "b1e402e594962946"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 教师模型微调",
   "id": "62e7b2d761c0a355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 定义要使用的预训练模型ID\n",
    "model_id = 'microsoft/deberta-v3-base' # 指定了强大的 DeBERTa V3 Base 模型\n",
    "# 2. 加载对应的分词器 (Tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# 3. 加载预训练模型用于序列分类任务\n",
    "# AutoModelForSequenceClassification 会自动加载适合序列分类的模型结构\n",
    "# 并在 DeBERTa 结构上添加一个分类头 (classification head)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ],
   "id": "8be3992e100dda40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained('deberta-v3-base-finetuned')\n",
    "tokenizer.save_pretrained('deberta-v3-base-finetuned') # 分词器也一起保存，方便后续使用"
   ],
   "id": "8fc55aef69fc65b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 创建并初始化学生模型\n",
    "\n",
    "现在我们有了教师模型 (deberta-v3-base-finetuned)，接下来要创建一个更小的学生模型，并用教师模型的权重来初始化它。"
   ],
   "id": "3863a205a53dc812"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. 定义创建学生模型的函数 create_student",
   "id": "860fce2c52c2e2e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义权重拷贝函数\n",
    "def copy_deberta_weights(teacher, student):\n",
    "    #权重拷贝逻辑，见下方\n",
    "    if isinstance(teacher, DebertaV2Model) or type(teacher).__name__.startswith('DebertaV2For'): # 检查是否是DeBERTa模型或其变体 (如序列分类模型)\n",
    "        # 递归地对模型的直接子模块进行拷贝\n",
    "        for teacher_part, student_part in zip(teacher.children(), student.children()):\n",
    "            copy_deberta_weights(teacher_part, student_part) # 对子模块递归调用\n",
    "\n",
    "    elif isinstance(teacher, DebertaV2Encoder): # 如果当前部分是 DeBERTa 的编码器\n",
    "        # 提取教师模型的编码层列表\n",
    "        teacher_encoding_layers = [layer for layer in next(teacher.children())] # next(teacher.children()) 获取编码器内部的层模块\n",
    "        # 提取学生模型的编码层列表\n",
    "        student_encoding_layers = [layer for layer in next(student.children())]\n",
    "        # 关键：拷贝部分层权重\n",
    "        # 学生模型的第 i 层，拷贝教师模型的第 2*i 层权重\n",
    "        for i in range(len(student_encoding_layers)):\n",
    "            student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict())\n",
    "\n",
    "    else: # 对于其他非特定处理的模块 (如 Embedding 层, 分类头等)\n",
    "        # 直接加载教师模型的权重字典到学生模型的对应模块\n",
    "        student.load_state_dict(teacher.state_dict())"
   ],
   "id": "f40dcb44e2cd41e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*   它首先检查传入的 `teacher` 和 `student` 部分是否是 DeBERTa 的整体模型或其变体。如果是，它就递归地对模型的子模块（如 embeddings, encoder）调用自身，确保深入到每一部分。\n",
    "*   当递归到 `DebertaV2Encoder`（编码器）部分时，它执行关键的层拷贝操作：\n",
    "    *   它获取教师和学生编码器内部的所有 Transformer 层。\n",
    "    *   然后，它遍历学生模型的每一层 (`i`)，将教师模型的**偶数层** (`2*i`) 的权重 (`state_dict`) 加载到学生模型的对应层中。例如，学生第 0 层加载教师第 0 层，学生第 1 层加载教师第 2 层，以此类推。这就是“使用教师模型的某些层次”的实现。\n",
    "*   对于模型的其他部分（比如词嵌入层 Embedding、最后的分类层 Classifier 等），它采用直接拷贝的方式，即将教师模型对应部分的 `state_dict` 直接加载到学生模型的对应部分。\n",
    "\n",
    "\n",
    "补充：在 Hugging Face 的 Transformers 库中，DebertaV2Model 是 基础模型，而 DebertaV2For... 是一些 在基础模型上加了任务头（head） 的模型。\n",
    "\n",
    "`teacher.children()` 和 `student.children()` 是 PyTorch 中 `nn.Module` 类（几乎所有的模型和层都继承自它）提供的方法。\n",
    "\n",
    "它的作用是**返回一个迭代器 (iterator)，这个迭代器包含了该模块的直接子模块 (direct child modules)**。\n",
    "\n",
    "**举个例子：**\n",
    "\n",
    "假设你的 `teacher_model` (它是一个 `nn.Module`) 的结构大致如下：\n",
    "\n",
    "```\n",
    "DebertaV2ForSequenceClassification(\n",
    "  (deberta): DebertaV2Model(\n",
    "    (embeddings): DebertaV2Embeddings(...)\n",
    "    (encoder): DebertaV2Encoder(...)\n",
    "    ...\n",
    "  )\n",
    "  (pooler): ContextPooler(...)\n",
    "  (classifier): Linear(...)\n",
    "  (dropout): StableDropout(...)\n",
    ")\n",
    "```\n",
    "\n",
    "当你调用 `teacher_model.children()` 时，它会返回一个迭代器，依次产生：\n",
    "\n",
    "1.  `DebertaV2Model` 对象 (名为 `deberta`)\n",
    "2.  `ContextPooler` 对象 (名为 `pooler`)\n",
    "3.  `Linear` 对象 (名为 `classifier`)\n",
    "4.  `StableDropout` 对象 (名为 `dropout`)\n",
    "\n",
    "**它只会返回直接的子模块。** 它不会返回孙子模块。例如，它不会直接返回 `DebertaV2Encoder` 里面的 Transformer 层，因为 `DebertaV2Encoder` 本身是 `DebertaV2Model` 的子模块，而不是 `DebertaV2ForSequenceClassification` 的直接子模块。\n"
   ],
   "id": "2cea794ae6ed0171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义创建学生模型的函数\n",
    "def create_student(teacher_model):\n",
    "    # a. 获取教师模型的配置，并转换为字典\n",
    "    configuration = teacher_model.config.to_dict()\n",
    "\n",
    "    # b. 修改配置：将隐藏层数量减半 (// 是整数除法)\n",
    "    original_num_layers = configuration[\"num_hidden_layers\"]\n",
    "    configuration[\"num_hidden_layers\"] //= 2\n",
    "    print(f\"Teacher layers: {original_num_layers}, Student layers: {configuration['num_hidden_layers']}\") # 打印层数变化\n",
    "\n",
    "    # c. 使用修改后的配置创建 DebertaV2Config 对象\n",
    "    student_config = DebertaV2Config.from_dict(configuration)\n",
    "\n",
    "    # d. 使用学生配置创建学生模型实例\n",
    "    # type(teacher_model) 获取教师模型的类 (例如 AutoModelForSequenceClassification)\n",
    "    # 然后用这个类和学生配置来创建学生模型，确保结构兼容（除了层数）\n",
    "    student_model = type(teacher_model)(config=student_config) # 注意这里用 config= 传递配置\n",
    "\n",
    "    # e. 调用权重拷贝函数，将教师的部分权重初始化给学生模型\n",
    "    copy_deberta_weights(teacher_model, student_model)\n",
    "\n",
    "    # f. 返回创建并初始化好的学生模型\n",
    "    return student_model"
   ],
   "id": "4e9f4eca14fb381d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "你可以把它想象成：我们先拿到教师模型的设计蓝图 (`.config`)，然后把它复印一份变成我们能随意涂改的草图 (`.to_dict()`)，接着我们在这份草图上修改我们想要简化的部分（比如减少楼层数），最后拿着修改后的草图去盖一栋新楼（学生模型）。",
   "id": "d65998af811371a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "初始化 Hugging Face 模型主要有两种方式：\n",
    "\n",
    "1.  **使用类的构造函数 `__init__` 并传入 `config` 对象:**\n",
    "    *   **代码示例:** `model = DebertaV2ForSequenceClassification(config=my_config)` 或我们例子中的 `student_model = type(teacher_model)(config=student_config)`\n",
    "    *   **作用:** 根据你提供的 `config` 对象中定义的**架构参数**（如层数、隐藏单元数等）来**从零开始构建模型骨架**。\n",
    "    *   **权重:** 这种方式创建的模型，其内部的**权重是随机初始化**的。它**不会**加载任何预训练好的权重。\n",
    "    *   **何时使用:**\n",
    "        *   当你想要训练一个**全新的模型**，完全不依赖预训练权重时。\n",
    "        *   当你修改了模型的架构（比如改变层数），需要根据这个**新的架构**创建一个模型实例时（就像我们为学生模型所做的）。创建之后，你可能需要手动加载部分权重（如我们的 `copy_deberta_weights`）或开始全新的训练。\n",
    "\n",
    "2.  **使用类方法 `from_pretrained()`:**\n",
    "    *   **代码示例:** `model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base')` 或 `teacher_model = AutoModelForSequenceClassification.from_pretrained('./deberta-v3-base-finetuned')`\n",
    "    *   **作用:** 从一个指定的**来源**（Hugging Face Hub 上的模型名称或本地保存模型的路径）加载**模型架构**和**对应的预训练权重**。\n",
    "    *   **权重:** 这是与第一种方式最关键的区别——`from_pretrained` 会自动**加载**与该模型关联的**已训练好的权重**（无论是初始的预训练权重还是微调后的权重）。\n",
    "    *   **配置:** `from_pretrained` 会自动从来源加载 `config.json` 文件来确定模型架构。你也可以在调用 `from_pretrained` 时传入 `config` 参数来覆盖某些配置，但这不太常见，主要目的是加载权重。\n",
    "    *   **何时使用:**\n",
    "        *   当你想要利用强大的**预训练模型**作为起点进行微调时（这是最常见的用法）。\n",
    "        *   当你需要加载之前**保存好**的、已经微调过的模型（比如我们加载教师模型）或训练过程中的检查点时。\n",
    "\n",
    "**总结关键区别:**\n",
    "\n",
    "| 特性         | `__init__(config=...)`                  | `from_pretrained(...)`                       |\n",
    "| :----------- | :--------------------------------------- | :------------------------------------------- |\n",
    "| **输入**     | 必须提供一个 `config` 对象             | 提供模型标识符 (str) 或本地路径 (str)        |\n",
    "| **架构来源** | 完全由传入的 `config` 对象决定         | 从来源加载 `config.json` 文件决定          |\n",
    "| **权重来源** | **随机初始化**                           | **从来源加载已保存的权重**                 |\n",
    "| **主要目的** | 构建指定架构的模型骨架 (无预训练权重) | 加载完整的预训练或已保存模型 (带权重)      |\n",
    "\n",
    "在我们的知识蒸馏场景中：\n",
    "\n",
    "*   我们用 `from_pretrained` 加载**教师模型**，因为它需要包含微调后的知识（权重）。\n",
    "*   我们用 `type(teacher_model)(config=student_config)`（本质是 `__init__`）来创建**学生模型**的骨架，因为它需要一个**新的、层数减少的架构**，权重是随后通过 `copy_deberta_weights` 部分初始化的，而不是直接加载某个预训练版本。"
   ],
   "id": "fc05de33ed847eca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 调用函数并保存学生模型",
   "id": "193a014af1a536ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# a. 加载之前保存的微调好的教师模型\n",
    "teacher_model_path = 'deberta-v3-base-finetuned'\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_path)\n",
    "\n",
    "# b. 调用 create_student 函数创建学生模型\n",
    "student_model = create_student(teacher_model)\n",
    "\n",
    "# c. 定义学生模型保存路径\n",
    "student_model_path = 'deberta-v3-base-student'\n",
    "\n",
    "# d. 保存初始化后的学生模型 (只保存模型本身，分词器后面会用教师的)\n",
    "student_model.save_pretrained(student_model_path)\n",
    "\n",
    "# (可选) 打印学生模型结构，确认层数是否减少\n",
    "print(student_model)"
   ],
   "id": "2c702e1cf4724e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 设置蒸馏训练",
   "id": "c22f04b65b4dc5ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "定义蒸馏训练参数类 DistillationTrainingArguments。\n",
    "这个类继承自 Hugging Face 的 TrainingArguments，并添加了 alpha 和 temperature 两个额外的参数。"
   ],
   "id": "e255fb67cbccdb3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义一个类，继承自 TrainingArguments，用于存放蒸馏相关的超参数\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        # 调用父类的构造函数，传递常规的训练参数\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # 初始化蒸馏特定的参数\n",
    "        self.alpha = alpha           # alpha 控制硬标签损失 (student_loss) 的权重\n",
    "        # (1 - alpha) 控制软标签损失 (distillation_loss) 的权重\n",
    "        self.temperature = temperature # temperature 用于计算软标签和软预测时的温度"
   ],
   "id": "54c8d9c972373447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义自定义蒸馏训练器 DistillationTrainer。这个类继承自 Hugging Face 的 Trainer，并重写了核心的 compute_loss 方法来实现蒸馏逻辑。",
   "id": "ae3bab0655cf9096"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义一个类，继承自 Trainer，用于执行蒸馏训练\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        # 调用父类的构造函数，传递常规参数 (包括 student_model 会被传给父类的 model 参数)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # 保存教师模型\n",
    "        self.teacher = teacher_model\n",
    "        # 确保教师模型和学生模型在同一个设备上 (CPU 或 GPU)\n",
    "        # self.model 是父类 Trainer 保存的学生模型\n",
    "        self._move_model_to_device(self.teacher, self.model.device)\n",
    "        # 将教师模型设置为评估模式，因为教师模型在蒸馏过程中不参与训练，只提供输出\n",
    "        self.teacher.eval()\n",
    "\n",
    "    # 重写 compute_loss 方法，这是蒸馏的核心\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        如何计算损失：\n",
    "        1. 计算学生模型的输出和标准损失 (硬损失)。\n",
    "        2. 在不计算梯度的情况下，计算教师模型的输出。\n",
    "        3. 使用 KL 散度计算学生软预测和教师软标签之间的蒸馏损失。\n",
    "        4. 使用 alpha 加权组合硬损失和蒸馏损失。\n",
    "        \"\"\"\n",
    "        # `model` 在这里就是学生模型 (因为它是传递给父类 Trainer 的 model)\n",
    "        # 1. 计算学生模型的输出\n",
    "        # **inputs 将输入的字典解包传递给模型\n",
    "        outputs_student = model(**inputs)\n",
    "\n",
    "        # 提取学生模型的原始损失 (通常是交叉熵损失，针对硬标签)\n",
    "        student_loss = outputs_student.loss\n",
    "        # 提取学生模型的 logits (softmax 前的原始输出)\n",
    "        logits_student = outputs_student.logits\n",
    "\n",
    "        # 2. 计算教师模型的输出 (在 no_grad 上下文中，不计算梯度)\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(**inputs)\n",
    "        # 提取教师模型的 logits\n",
    "        logits_teacher = outputs_teacher.logits\n",
    "\n",
    "        # 3. 计算蒸馏损失 (KL 散度)\n",
    "        #    a. 从训练参数中获取温度 T 和 alpha\n",
    "        temperature = self.args.temperature\n",
    "        alpha = self.args.alpha\n",
    "\n",
    "        #    b. 定义 KL 散度损失函数，reduction='batchmean' 表示对 batch 内样本的损失求平均\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "        #    c. 计算蒸馏损失\n",
    "        #       F.log_softmax(logits_student / temperature, dim=-1): 学生 logits 除以温度 T，然后取 log_softmax\n",
    "        #       F.softmax(logits_teacher / temperature, dim=-1):   教师 logits 除以温度 T，然后取 softmax (作为目标分布)\n",
    "        #       KLDivLoss 需要输入是 log 概率，目标是普通概率\n",
    "        distillation_loss = loss_fct(\n",
    "            F.log_softmax(logits_student / temperature, dim=-1),\n",
    "            F.softmax(logits_teacher / temperature, dim=-1)\n",
    "        )\n",
    "\n",
    "        #    d. 乘以温度的平方进行缩放 (Hinton 论文中的做法)\n",
    "        distillation_loss = distillation_loss * (temperature ** 2)\n",
    "\n",
    "        # 4. 组合损失\n",
    "        #    最终损失 = alpha * 硬损失 + (1 - alpha) * 蒸馏损失\n",
    "        loss = alpha * student_loss + (1.0 - alpha) * distillation_loss\n",
    "\n",
    "        # 根据 return_outputs 参数决定返回值\n",
    "        # 如果为 True，返回损失和学生模型的输出；否则只返回损失\n",
    "        return (loss, outputs_student) if return_outputs else loss"
   ],
   "id": "d6645ea2d93cc706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "*   **背景：** 当我们计算损失时，输入通常是一个**批次 (batch)** 的数据。例如，`logits_student` 和 `logits_teacher` 的形状可能是 `[batch_size, num_classes]`。计算 KL 散度会得到一个针对**每个样本**的损失值，所以我们会得到一个形状类似 `[batch_size]` 的张量，其中每个元素是对应样本的 KL 散度损失。\n",
    "*   **聚合 (Reduction)：** 为了进行反向传播和模型优化，我们通常需要将这一个批次的**所有样本的损失**聚合成一个**单一的标量值 (scalar)**。`reduction` 参数就是用来控制这个聚合方式的。\n",
    "*   **`\"batchmean\"` 的具体含义 (对于 `KLDivLoss`)：**\n",
    "    *   它指示 PyTorch 首先计算每个样本的 KL 散度损失。\n",
    "    *   然后，将这些损失值在**批次维度 (batch dimension) 上求平均值**。\n",
    "    *   (根据 `KLDivLoss` 的文档，它实际上是先在特征维度上求和，然后在批次维度上求平均。但最终效果是得到一个代表整个批次平均损失的标量值)。\n",
    "    *   **简单理解：** 计算出批次中每个样本的模仿损失后，取它们的平均值作为这个批次的最终模仿损失。\n",
    "\n",
    "`KLDivLoss` (以及许多其他 PyTorch 损失函数) 通常支持以下几种 `reduction` 模式：\n",
    "\n",
    "*   **`'mean'`:** 计算损失的平均值。对于 `KLDivLoss`，`'mean'` 和 `'batchmean'` 的效果通常是一样的（都是求批次的平均损失）。这是最常用的选项，因为它使得损失的大小不直接受批次大小的影响。\n",
    "*   **`'sum'`:** 计算损失的总和。即把批次中所有样本的损失加起来。如果使用这个，损失值会随着批次增大而增大，可能需要相应调整学习率。\n",
    "*   **`'none'`:** 不进行聚合。损失函数将返回一个与输入批次大小相同的张量，其中包含每个样本的损失值。这在需要对每个样本的损失进行单独处理时有用，但在标准训练循环中不直接用于反向传播（因为反向传播需要标量损失）。"
   ],
   "id": "8938578e59e3d10f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " `KLDivLoss` 通常期望的输入 (Input) 是对数概率 (log-probabilities)，而目标 (Target) 是普通概率 (probabilities)。这就是为什么在 `compute_loss` 函数中，我们对学生 logits 使用 `F.log_softmax`，而对教师 logits 使用 `F.softmax`。",
   "id": "fc725cd1439be200"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载模型、数据并进行蒸馏训练",
   "id": "421839324f1cd575"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 定义模型路径\n",
    "student_id = \"deberta-v3-base-student\" # 之前保存的学生模型路径\n",
    "teacher_id = \"deberta-v3-base-finetuned\" # 之前保存的教师模型路径\n",
    "\n",
    "# 2. 加载教师模型\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_id)\n",
    "# 3. 加载初始化好的学生模型\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(student_id)\n",
    "\n",
    "# 4. 加载分词器 (用教师的或学生的都可以，因为它们应该是一样的)\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_id) # 或者 student_id"
   ],
   "id": "bf3047d66f419116"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. 配置蒸馏训练参数\n",
    "output_dir = \"checkpoint\" # 训练过程中保存检查点的目录\n",
    "logging_dir = './logs'     # 保存 TensorBoard 日志的目录\n",
    "\n",
    "training_args = DistillationTrainingArguments(\n",
    "    output_dir=output_dir,               # 输出目录\n",
    "    num_train_epochs=3,                  # 训练轮数\n",
    "    per_device_train_batch_size=2,       # 每个 GPU/CPU 上的训练批次大小\n",
    "    per_device_eval_batch_size=4,        # 每个 GPU/CPU 上的评估批次大小\n",
    "    gradient_accumulation_steps=8,       # 梯度累积步数 (增大有效批次大小)\n",
    "    gradient_checkpointing=True,         # 使用梯度检查点节省显存 (会稍慢)\n",
    "    warmup_steps=500,                    # 学习率预热步数\n",
    "    weight_decay=0.01,                   # 权重衰减系数 (L2 正则化)\n",
    "    logging_dir=logging_dir,             # 日志目录\n",
    "    logging_steps=100,                   # 每隔多少步记录一次日志\n",
    "    evaluation_strategy=\"epoch\",         # 每个 epoch 结束时进行一次评估\n",
    "    save_strategy=\"no\",                  # 这里设置为不自动保存模型 (可以改为 \"epoch\" 按轮保存)\n",
    "    # --- 蒸馏特定参数 ---\n",
    "    alpha=0.5,                           # 硬损失和软损失的权重平衡因子\n",
    "    temperature=4.0                      # 软化标签的温度 (文档示例用了 4.0)\n",
    ")"
   ],
   "id": "b99372a33b54eb8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 6. 创建 DistillationTrainer 实例\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,                 # 学生模型传递给 'model' 参数\n",
    "    teacher_model=teacher_model,         # 教师模型传递给自定义的 'teacher_model' 参数\n",
    "    args=training_args,                  # 传入包含蒸馏参数的训练配置\n",
    "    train_dataset=tokenized_train,       # 训练数据集\n",
    "    eval_dataset=tokenized_val,          # 验证数据集\n",
    "    data_collator=data_collator,         # 数据整理器\n",
    "    tokenizer=tokenizer,                 # 分词器 (用于填充等)\n",
    "    compute_metrics=compute_metrics      # 评估指标计算函数\n",
    ")"
   ],
   "id": "e8bac9779e643bd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 7. 开始蒸馏训练\n",
    "print(\"Starting distillation training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")"
   ],
   "id": "280f5db47383c593"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (可选) 保存最终训练好的学生模型\n",
    "final_student_model_path = \"deberta-v3-base-student-distilled\"\n",
    "trainer.save_model(final_student_model_path)\n",
    "print(f\"Final distilled student model saved to {final_student_model_path}\")"
   ],
   "id": "2bcde98460bc6a9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# --- 假设 tokenized_test 和 test 数据集已准备好 ---\n",
    "# tokenized_test: 经过分词器处理的测试数据集 (Dataset 对象)\n",
    "# test: 原始测试数据集，包含 'id' 等信息，用于保存结果 (例如 Pandas DataFrame 或 Dataset 对象)\n",
    "\n",
    "# (示例：假设 test 是加载的原始数据集的一部分)\n",
    "# test = dataset[\"test\"].shard(num_shards=2, index=1) # 对应之前 tokenized_test 的原始数据\n",
    "# --- 数据准备结束 ---\n",
    "\n",
    "\n",
    "# 1. 使用训练好的 trainer 在测试集上进行预测\n",
    "# trainer 内部持有训练完成的学生模型\n",
    "print(\"Evaluating the distilled student model on the test set...\")\n",
    "prediction_outputs = trainer.predict(tokenized_test)\n",
    "\n",
    "# prediction_outputs 是一个 PredictionOutput 对象 (或类似结构)\n",
    "# prediction_outputs.predictions 包含了模型输出的 logits (通常是 prediction_outputs[0])\n",
    "# prediction_outputs.label_ids 包含了真实的标签 (通常是 prediction_outputs[1]，如果测试集有标签)\n",
    "# prediction_outputs.metrics 包含了计算得到的评估指标 (如果 compute_metrics 函数被调用)\n",
    "\n",
    "print(\"Prediction metrics:\", prediction_outputs.metrics) # 打印评估指标 (如准确率)\n",
    "\n",
    "# 2. 从预测输出中提取最终的预测标签\n",
    "# 获取模型的原始输出 logits (通常在第一个位置)\n",
    "logits = prediction_outputs.predictions # 或者 prediction_outputs[0]\n",
    "# 找到每个样本概率最高的类别索引作为预测标签\n",
    "test_pred = np.argmax(logits, axis=-1).flatten() # argmax 找最大值索引，flatten 转为一维数组\n",
    "\n",
    "# 3. (可选) 打印部分预测结果，直观感受\n",
    "print(\"Sample predictions (first 20):\", test_pred[:20])\n",
    "# 如果测试集有真实标签，可以比较一下\n",
    "# if prediction_outputs.label_ids is not None:\n",
    "#     true_labels = prediction_outputs.label_ids.flatten()\n",
    "#     print(\"Sample true labels (first 20):\", true_labels[:20])\n",
    "\n",
    "# 4. 将预测结果格式化并保存到文件\n",
    "# 创建一个 Pandas DataFrame 来存储结果\n",
    "# 假设原始 'test' 数据集里有 'id' 这一列\n",
    "# 如果 'test' 是 Dataset 对象，可能需要先转换为 Pandas: test_df = test.to_pandas()\n",
    "# 这里假设 test 是可以直接按索引访问 id 的结构，例如 test['id']\n",
    "# 请根据你的实际 test 数据结构调整 data={\"id\": ...} 部分\n",
    "try:\n",
    "    # 尝试假设 test 是字典或类似结构\n",
    "    result_data = {\"id\": test[\"id\"], \"sentiment\": test_pred}\n",
    "except TypeError:\n",
    "    # 如果 test 是 Dataset, 可能需要迭代获取 id\n",
    "    test_ids = [item['id'] for item in test] # 示例，具体字段名可能不同\n",
    "    result_data = {\"id\": test_ids, \"sentiment\": test_pred}\n",
    "\n",
    "result_output = pd.DataFrame(data=result_data)\n",
    "\n",
    "# 定义保存路径和文件名\n",
    "output_csv_path = \"./result/deberta_base_student_distilled.csv\" # 修改了文件名以区分\n",
    "# 将 DataFrame 保存为 CSV 文件\n",
    "# index=False 表示不将 DataFrame 的索引写入文件\n",
    "# quoting=3 (QUOTE_NONNUMERIC) 表示给非数字加上引号，符合某些提交格式要求\n",
    "result_output.to_csv(output_csv_path, index=False, quoting=3)\n",
    "\n",
    "logging.info(f'Result saved to {output_csv_path}!')\n",
    "print(f\"Predictions saved to {output_csv_path}\")\n",
    "\n",
    "# 5. 分析结果 (手动步骤)\n",
    "# - 查看保存的 CSV 文件。\n",
    "# - 将 prediction_outputs.metrics 中的指标与教师模型在该测试集上的指标进行比较。\n",
    "# - 分析学生模型在哪些样本上表现好/差。\n",
    "# - 根据文档建议，可以尝试不同的学生模型、alpha、temperature 值，重复训练和评估过程，找到最佳组合。"
   ],
   "id": "ad7672043e39a613"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
